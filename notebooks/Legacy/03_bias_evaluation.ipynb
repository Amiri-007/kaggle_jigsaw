{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0193afd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Jigsaw Unintended Bias Audit: Fairness Metrics Evaluation\n",
    "# \n",
    "# This notebook evaluates bias metrics across different demographic subgroups for toxicity classification models.\n",
    "# \n",
    "# **Papermill Parameters:**\n",
    "# - `model_name`: Name of the model to evaluate (default: \"tfidf_logreg\")\n",
    "\n",
    "# ## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d1d5eb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Parameters for papermill\n",
    "model_name = \"tfidf_logreg\"  # Default model name, can be overridden by papermill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705e30a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "# Import our metrics module\n",
    "import src.metrics_v2 as metrics_v2\n",
    "\n",
    "# Set up directories\n",
    "DATA_DIR = \"../data\"\n",
    "PREDS_DIR = \"../output/preds\"\n",
    "RESULTS_DIR = \"../results\"\n",
    "FIGURES_DIR = \"../output/figures\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "\n",
    "# ## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c480565",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_ground_truth():\n",
    "    \"\"\"Load ground truth data with identity columns.\"\"\"\n",
    "    # First try test data (for real evaluation)\n",
    "    test_file = os.path.join(DATA_DIR, \"test_public_expanded.csv\")\n",
    "    if os.path.exists(test_file):\n",
    "        print(f\"Loading test data from {test_file}\")\n",
    "        return pd.read_csv(test_file)\n",
    "    \n",
    "    # Fall back to train data\n",
    "    train_file = os.path.join(DATA_DIR, \"train.csv\")\n",
    "    if os.path.exists(train_file):\n",
    "        print(f\"Loading train data from {train_file}\")\n",
    "        return pd.read_csv(train_file)\n",
    "    \n",
    "    raise FileNotFoundError(\"No ground truth data found. Please add train.csv or test_public_expanded.csv to the data directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4478da9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_predictions(model_name):\n",
    "    \"\"\"Load model predictions.\"\"\"\n",
    "    pred_file = os.path.join(PREDS_DIR, f\"{model_name}.csv\")\n",
    "    if not os.path.exists(pred_file):\n",
    "        # Try looking in the results directory\n",
    "        pred_file = os.path.join(RESULTS_DIR, f\"preds_{model_name}.csv\")\n",
    "        if not os.path.exists(pred_file):\n",
    "            raise FileNotFoundError(f\"Predictions file not found for model: {model_name}\")\n",
    "    \n",
    "    print(f\"Loading predictions from {pred_file}\")\n",
    "    return pd.read_csv(pred_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b38ae3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load ground truth data\n",
    "ground_truth = load_ground_truth()\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Ground truth data shape: {ground_truth.shape}\")\n",
    "print(f\"Columns: {', '.join(ground_truth.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f282b5cd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Identify identity columns\n",
    "# Exclude standard non-identity columns\n",
    "non_identity_cols = [\n",
    "    'id', 'comment_text', 'target', 'toxicity', 'severe_toxicity', \n",
    "    'obscene', 'threat', 'insult', 'sexual_explicit'\n",
    "]\n",
    "identity_cols = [col for col in ground_truth.columns if col not in non_identity_cols]\n",
    "\n",
    "print(f\"Identified {len(identity_cols)} identity columns: {', '.join(identity_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37939b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load predictions for the specified model\n",
    "predictions = load_predictions(model_name)\n",
    "print(f\"Prediction data shape: {predictions.shape}\")\n",
    "\n",
    "\n",
    "# ## Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073f7dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Merge ground truth with predictions\n",
    "merged_data = pd.merge(ground_truth, predictions, on='id')\n",
    "print(f\"Merged data shape: {merged_data.shape}\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_count = merged_data.isnull().sum().sum()\n",
    "if missing_count > 0:\n",
    "    print(f\"Warning: {missing_count} missing values found in merged data\")\n",
    "    print(merged_data.isnull().sum()[merged_data.isnull().sum() > 0])\n",
    "\n",
    "\n",
    "# ## Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbca4cb4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Extract arrays\n",
    "y_true = merged_data['target'].values\n",
    "y_pred = merged_data['prediction'].values\n",
    "\n",
    "# Create subgroup masks\n",
    "subgroup_masks = {}\n",
    "for col in identity_cols:\n",
    "    subgroup_masks[col] = merged_data[col].values.astype(bool)\n",
    "\n",
    "# Calculate overall AUC\n",
    "overall_auc = roc_auc_score(y_true, y_pred)\n",
    "print(f\"Overall AUC: {overall_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f949b5b2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "print(f\"Calculating bias metrics for {len(identity_cols)} identity subgroups...\")\n",
    "results = metrics_v2.compute_all_metrics(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_pred,\n",
    "    subgroup_masks=subgroup_masks,\n",
    "    power=-5,            # Power parameter for generalized mean\n",
    "    weight_overall=0.25  # Weight for overall AUC in final score\n",
    ")\n",
    "\n",
    "print(f\"Bias calculation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254c5111",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a DataFrame with subgroup metrics\n",
    "subgroup_metrics_df = pd.DataFrame(results[\"subgroup_metrics\"])\n",
    "\n",
    "# Add overall metrics\n",
    "print(f\"Overall AUC: {results['overall']['auc']:.4f}\")\n",
    "print(f\"Final Score: {results['overall']['final_score']:.4f}\")\n",
    "\n",
    "# Display power means\n",
    "for key, value in results[\"bias_metrics\"].items():\n",
    "    if key.startswith(\"power_mean\"):\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "\n",
    "# ## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6971ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Save metrics to CSV\n",
    "metrics_file = os.path.join(RESULTS_DIR, f\"metrics_{model_name}.csv\")\n",
    "subgroup_metrics_df.to_csv(metrics_file, index=False)\n",
    "print(f\"Saved metrics to {metrics_file}\")\n",
    "\n",
    "# Save predictions to results directory for easier access by the dashboard\n",
    "pred_file = os.path.join(RESULTS_DIR, f\"preds_{model_name}.csv\")\n",
    "if not os.path.exists(pred_file):\n",
    "    predictions.to_csv(pred_file, index=False)\n",
    "    print(f\"Copied predictions to {pred_file}\")\n",
    "\n",
    "\n",
    "# ## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6699e764",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import visualization utilities\n",
    "from src.vis_utils import plot_auc_heatmap, plot_threshold_sweep\n",
    "\n",
    "# Create heatmap\n",
    "fig = plot_auc_heatmap(\n",
    "    metrics_file,\n",
    "    title=f\"Bias Metrics for {model_name}\",\n",
    "    save_path=os.path.join(FIGURES_DIR, f\"heatmap_{model_name}.svg\")\n",
    ")\n",
    "\n",
    "# Display the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf5a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create threshold sweep for a sample identity column\n",
    "sample_identity = identity_cols[0]\n",
    "sample_mask = subgroup_masks[sample_identity]\n",
    "\n",
    "fig = plot_threshold_sweep(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    sample_mask,\n",
    "    sample_identity,\n",
    "    save_path=os.path.join(FIGURES_DIR, f\"threshold_sweep_{model_name}_{sample_identity}.svg\")\n",
    ")\n",
    "\n",
    "# Display the figure\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# ## Create Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c779ba",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a summary report with the key metrics\n",
    "report = {\n",
    "    \"model_name\": model_name,\n",
    "    \"overall_auc\": results[\"overall\"][\"auc\"],\n",
    "    \"final_score\": results[\"overall\"][\"final_score\"],\n",
    "    \"power_mean_subgroup_auc\": results[\"bias_metrics\"][\"power_mean_subgroup_auc\"],\n",
    "    \"power_mean_bpsn_auc\": results[\"bias_metrics\"][\"power_mean_bpsn_auc\"],\n",
    "    \"power_mean_bnsp_auc\": results[\"bias_metrics\"][\"power_mean_bnsp_auc\"],\n",
    "    \"subgroups_evaluated\": len(identity_cols),\n",
    "    \"worst_subgroup_auc\": subgroup_metrics_df[\"subgroup_auc\"].min(),\n",
    "    \"worst_subgroup\": subgroup_metrics_df.loc[subgroup_metrics_df[\"subgroup_auc\"].idxmin(), \"subgroup_name\"]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame for better display\n",
    "report_df = pd.DataFrame([report])\n",
    "report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda80f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Save summary report\n",
    "report_file = os.path.join(RESULTS_DIR, f\"summary_{model_name}.csv\")\n",
    "report_df.to_csv(report_file, index=False)\n",
    "print(f\"Saved summary report to {report_file}\")\n",
    "\n",
    "print(\"\\nEvaluation complete!\") "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
