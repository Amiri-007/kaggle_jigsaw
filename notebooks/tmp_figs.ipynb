{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b8ec3af",
   "metadata": {},
   "source": [
    "# Fairness Figures Generation\n",
    "\n",
    "This notebook generates a complete set of publication-ready fairness figures for each model whose metrics CSV lives in results/. All images are saved under the `figs/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73e3f4f",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dcc606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from src import figure_utils\n",
    "\n",
    "# Create figs directory if it doesn't exist\n",
    "os.makedirs('figs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f2f53f",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2a4020",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "TRAIN_CSV = \"data/train.csv\"\n",
    "TEST_CSV = \"data/test_public_expanded.csv\"\n",
    "RESULTS_DIR = \"results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90d46c0",
   "metadata": {},
   "source": [
    "## Utility Function to List Identity Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08357d5b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def list_identity_columns(df):\n",
    "    \"\"\"\n",
    "    List all identity columns in the dataframe.\n",
    "    \n",
    "    Identity columns are typically those related to demographic groups.\n",
    "    In the Jigsaw dataset, these are columns like 'male', 'female', 'black', etc.\n",
    "    \"\"\"\n",
    "    # Common identity column patterns\n",
    "    identity_patterns = [\n",
    "        # Demographic groups\n",
    "        'male', 'female', 'transgender', 'other_gender', 'heterosexual', 'homosexual_gay_or_lesbian',\n",
    "        'bisexual', 'other_sexual_orientation', 'christian', 'jewish', 'muslim', 'hindu',\n",
    "        'buddhist', 'atheist', 'other_religion', 'black', 'white', 'asian', 'latino',\n",
    "        'other_race_or_ethnicity', 'physical_disability', 'intellectual_or_learning_disability',\n",
    "        'psychiatric_or_mental_illness', 'other_disability',\n",
    "        \n",
    "        # Sometimes these are prefixed\n",
    "        'identity_', 'demo_'\n",
    "    ]\n",
    "    \n",
    "    identity_cols = []\n",
    "    \n",
    "    # Check each column in the dataframe\n",
    "    for col in df.columns:\n",
    "        # Check if the column matches any of the identity patterns\n",
    "        if any(pattern in col.lower() for pattern in identity_patterns):\n",
    "            identity_cols.append(col)\n",
    "    \n",
    "    return identity_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9379efb3",
   "metadata": {},
   "source": [
    "## Generate Figure 1: Identity Prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e498ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "# Get identity columns\n",
    "identity_cols = list_identity_columns(train_df)\n",
    "print(f\"Found {len(identity_cols)} identity columns: {identity_cols}\")\n",
    "\n",
    "# Generate the identity prevalence figure\n",
    "prevalence_fig = figure_utils.identity_prevalence(train_df, identity_cols)\n",
    "plt.close(prevalence_fig)  # Close the figure to free memory\n",
    "\n",
    "print(f\"Saved identity prevalence figure to figs/identity_prevalence.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe14610",
   "metadata": {},
   "source": [
    "## Process Each Model's Results and Generate Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee894f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all metrics files in the results directory\n",
    "metrics_files = glob.glob(os.path.join(RESULTS_DIR, \"metrics_*.csv\"))\n",
    "print(f\"Found {len(metrics_files)} metrics files\")\n",
    "\n",
    "# Initialize a list to store figure metadata\n",
    "figure_inventory = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fa2e14",
   "metadata": {},
   "source": [
    "## Loop Through Each Model's Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2fcfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metrics_file in metrics_files:\n",
    "    # Extract model name from the filename\n",
    "    model_name = os.path.basename(metrics_file).replace(\"metrics_\", \"\").replace(\".csv\", \"\")\n",
    "    print(f\"Processing model: {model_name}\")\n",
    "    \n",
    "    # Load metrics\n",
    "    try:\n",
    "        metrics_df = pd.read_csv(metrics_file)\n",
    "        print(f\"Loaded metrics data with shape: {metrics_df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading metrics file {metrics_file}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Check if the metrics file has the expected structure\n",
    "    required_columns = ['identity_group', 'metric_name', 'value']\n",
    "    if not all(col in metrics_df.columns for col in required_columns):\n",
    "        # If not, try to reshape the data to match the expected format\n",
    "        print(f\"Metrics file doesn't have the expected columns: {required_columns}\")\n",
    "        print(\"Attempting to reshape data...\")\n",
    "        \n",
    "        # Melt the DataFrame to convert it to the required format\n",
    "        try:\n",
    "            # Assuming first column is identity_group and other columns are metrics\n",
    "            id_col = metrics_df.columns[0]\n",
    "            metric_cols = metrics_df.columns[1:]\n",
    "            \n",
    "            metrics_df = pd.melt(\n",
    "                metrics_df, \n",
    "                id_vars=[id_col], \n",
    "                value_vars=metric_cols,\n",
    "                var_name='metric_name',\n",
    "                value_name='value'\n",
    "            )\n",
    "            metrics_df = metrics_df.rename(columns={id_col: 'identity_group'})\n",
    "            print(f\"Reshaped metrics data to: {metrics_df.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reshaping metrics data: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Load predictions if available\n",
    "    pred_file = os.path.join(RESULTS_DIR, f\"preds_{model_name}.csv\")\n",
    "    preds_df = None\n",
    "    \n",
    "    if os.path.exists(pred_file):\n",
    "        try:\n",
    "            preds_df = pd.read_csv(pred_file)\n",
    "            print(f\"Loaded predictions data with shape: {preds_df.shape}\")\n",
    "            \n",
    "            # Check if the predictions file has the expected structure (id, prediction)\n",
    "            if 'id' in preds_df.columns and 'prediction' in preds_df.columns:\n",
    "                pass\n",
    "            elif len(preds_df.columns) >= 2:\n",
    "                # Rename the columns to match expected format\n",
    "                preds_df = preds_df.iloc[:, :2]\n",
    "                preds_df.columns = ['id', 'prediction']\n",
    "            else:\n",
    "                print(f\"Predictions file doesn't have the expected structure\")\n",
    "                preds_df = None\n",
    "                \n",
    "            # If test_expanded file exists, merge with predictions\n",
    "            if os.path.exists(TEST_CSV) and preds_df is not None:\n",
    "                test_df = pd.read_csv(TEST_CSV)\n",
    "                if 'id' in test_df.columns:\n",
    "                    preds_df = pd.merge(preds_df, test_df, on='id', how='left')\n",
    "                    print(f\"Merged predictions with test data: {preds_df.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading predictions file {pred_file}: {e}\")\n",
    "            preds_df = None\n",
    "    \n",
    "    # Generate Figure 2: ROC Curve\n",
    "    if preds_df is not None and 'prediction' in preds_df.columns and 'target' in preds_df.columns:\n",
    "        roc_fig = figure_utils.roc_curve_figure(\n",
    "            preds_df['target'], \n",
    "            preds_df['prediction'], \n",
    "            model_name\n",
    "        )\n",
    "        plt.close(roc_fig)  # Close the figure to free memory\n",
    "        figure_inventory.append(f\"overall_roc_{model_name}.png\")\n",
    "        print(f\"Generated ROC curve figure for {model_name}\")\n",
    "    else:\n",
    "        print(f\"Skipping ROC curve figure for {model_name} (missing predictions or target)\")\n",
    "    \n",
    "    # Generate Figure 3: Fairness Heatmap\n",
    "    heatmap_fig = figure_utils.fairness_heatmap(metrics_df, model_name)\n",
    "    plt.close(heatmap_fig)  # Close the figure to free memory\n",
    "    figure_inventory.append(f\"fairness_heatmap_{model_name}.png\")\n",
    "    print(f\"Generated fairness heatmap figure for {model_name}\")\n",
    "    \n",
    "    # Generate Figure 4: Power Mean Bar\n",
    "    power_fig = figure_utils.power_mean_bar(metrics_df, model_name)\n",
    "    plt.close(power_fig)  # Close the figure to free memory\n",
    "    figure_inventory.append(f\"power_mean_bar_{model_name}.png\")\n",
    "    print(f\"Generated power mean bar figure for {model_name}\")\n",
    "    \n",
    "    # Generate Figure 5: Threshold Sweep\n",
    "    if preds_df is not None and 'prediction' in preds_df.columns and 'target' in preds_df.columns:\n",
    "        # Extract identity columns from the test data\n",
    "        test_identity_cols = list_identity_columns(preds_df)\n",
    "        \n",
    "        if test_identity_cols:\n",
    "            # Create identity arrays for the threshold sweep\n",
    "            identity_arrays = [preds_df[col].values for col in test_identity_cols]\n",
    "            \n",
    "            threshold_fig = figure_utils.threshold_sweep(\n",
    "                preds_df['target'],\n",
    "                preds_df['prediction'],\n",
    "                identity_arrays,\n",
    "                model_name\n",
    "            )\n",
    "            plt.close(threshold_fig)  # Close the figure to free memory\n",
    "            figure_inventory.append(f\"threshold_sweep_{model_name}.png\")\n",
    "            print(f\"Generated threshold sweep figure for {model_name}\")\n",
    "        else:\n",
    "            print(f\"Skipping threshold sweep figure for {model_name} (no identity columns in test data)\")\n",
    "    else:\n",
    "        print(f\"Skipping threshold sweep figure for {model_name} (missing predictions or target)\")\n",
    "    \n",
    "    # Generate Figure 6: Worst K Table\n",
    "    worst_k_fig = figure_utils.worst_k_table(metrics_df, k=5, model_name=model_name)\n",
    "    plt.close(worst_k_fig)  # Close the figure to free memory\n",
    "    figure_inventory.append(f\"worst_k_table_{model_name}.png\")\n",
    "    print(f\"Generated worst k table figure for {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73393f49",
   "metadata": {},
   "source": [
    "## Generate Figure 7: Before vs After Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8508233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find baseline and improved model metrics\n",
    "baseline_metrics_file = None\n",
    "improved_metrics_file = None\n",
    "\n",
    "# Look for baseline model metrics\n",
    "for metrics_file in metrics_files:\n",
    "    if \"baseline\" in metrics_file.lower():\n",
    "        baseline_metrics_file = metrics_file\n",
    "        break\n",
    "\n",
    "# If no explicit baseline, use the first metrics file\n",
    "if baseline_metrics_file is None and metrics_files:\n",
    "    baseline_metrics_file = metrics_files[0]\n",
    "\n",
    "# Look for improved model metrics (tfidf_lr_full is specified in the requirements)\n",
    "for metrics_file in metrics_files:\n",
    "    if \"tfidf_lr_full\" in metrics_file.lower():\n",
    "        improved_metrics_file = metrics_file\n",
    "        break\n",
    "\n",
    "# If no explicit improved model, use the last metrics file (different from baseline)\n",
    "if improved_metrics_file is None and len(metrics_files) > 1:\n",
    "    improved_metrics_file = [f for f in metrics_files if f != baseline_metrics_file][0]\n",
    "\n",
    "if baseline_metrics_file and improved_metrics_file and baseline_metrics_file != improved_metrics_file:\n",
    "    print(f\"Generating before vs after scatter plot:\")\n",
    "    print(f\"  Baseline: {os.path.basename(baseline_metrics_file)}\")\n",
    "    print(f\"  Improved: {os.path.basename(improved_metrics_file)}\")\n",
    "    \n",
    "    # Load the metrics DataFrames\n",
    "    baseline_df = pd.read_csv(baseline_metrics_file)\n",
    "    improved_df = pd.read_csv(improved_metrics_file)\n",
    "    \n",
    "    # Check if the metrics files have the expected structure\n",
    "    required_columns = ['identity_group', 'metric_name', 'value']\n",
    "    \n",
    "    for df, name in [(baseline_df, \"baseline\"), (improved_df, \"improved\")]:\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            # If not, try to reshape the data to match the expected format\n",
    "            print(f\"{name} metrics file doesn't have the expected columns: {required_columns}\")\n",
    "            print(\"Attempting to reshape data...\")\n",
    "            \n",
    "            # Melt the DataFrame to convert it to the required format\n",
    "            try:\n",
    "                # Assuming first column is identity_group and other columns are metrics\n",
    "                id_col = df.columns[0]\n",
    "                metric_cols = df.columns[1:]\n",
    "                \n",
    "                reshaped_df = pd.melt(\n",
    "                    df, \n",
    "                    id_vars=[id_col], \n",
    "                    value_vars=metric_cols,\n",
    "                    var_name='metric_name',\n",
    "                    value_name='value'\n",
    "                )\n",
    "                reshaped_df = reshaped_df.rename(columns={id_col: 'identity_group'})\n",
    "                \n",
    "                if name == \"baseline\":\n",
    "                    baseline_df = reshaped_df\n",
    "                else:\n",
    "                    improved_df = reshaped_df\n",
    "                    \n",
    "                print(f\"Reshaped {name} metrics data successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reshaping {name} metrics data: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Get common identity groups\n",
    "    baseline_identity_groups = baseline_df['identity_group'].unique()\n",
    "    improved_identity_groups = improved_df['identity_group'].unique()\n",
    "    common_identity_groups = list(set(baseline_identity_groups) & set(improved_identity_groups))\n",
    "    \n",
    "    if common_identity_groups:\n",
    "        scatter_fig = figure_utils.before_vs_after_scatter(\n",
    "            baseline_df,\n",
    "            improved_df,\n",
    "            common_identity_groups\n",
    "        )\n",
    "        plt.close(scatter_fig)  # Close the figure to free memory\n",
    "        figure_inventory.append(\"before_vs_after_scatter.png\")\n",
    "        print(f\"Generated before vs after scatter plot\")\n",
    "    else:\n",
    "        print(\"No common identity groups found between baseline and improved models\")\n",
    "else:\n",
    "    print(\"Skipping before vs after scatter plot (couldn't find suitable baseline and improved models)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3208e7e4",
   "metadata": {},
   "source": [
    "## Save Figure Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f39bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the figure inventory\n",
    "inventory = {\n",
    "    \"figures\": figure_inventory,\n",
    "    \"count\": len(figure_inventory)\n",
    "}\n",
    "\n",
    "with open('figs/figure_inventory.json', 'w') as f:\n",
    "    json.dump(inventory, f, indent=2)\n",
    "\n",
    "print(f\"Saved figure inventory to figs/figure_inventory.json\")\n",
    "print(f\"Generated {len(figure_inventory)} figures in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8af563",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has generated the following figures:\n",
    "\n",
    "1. `identity_prevalence.png` - Bar plot showing prevalence of identity groups\n",
    "2. `overall_roc_{model}.png` - ROC curves for each model\n",
    "3. `fairness_heatmap_{model}.png` - Heatmaps of fairness metrics for each model\n",
    "4. `power_mean_bar_{model}.png` - Bar plots of power mean differences for each model\n",
    "5. `threshold_sweep_{model}.png` - Threshold sweep analysis for each model\n",
    "6. `worst_k_table_{model}.png` - Tables of worst-performing identity groups for each model\n",
    "7. `before_vs_after_scatter.png` - Scatter plot comparing baseline and improved models\n",
    "\n",
    "All figures are saved in the `figs/` directory. "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "executable": "/usr/bin/env python",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
