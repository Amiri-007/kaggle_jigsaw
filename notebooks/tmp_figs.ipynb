{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9557c5ea",
   "metadata": {},
   "source": [
    "# Fairness Figures Generation\n",
    "\n",
    "This notebook generates a complete set of publication-ready fairness figures for each model whose metrics CSV lives in results/. All images are saved under the `figs/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6a50bb",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd555b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from src import figure_utils, threshold_utils\n",
    "from pathlib import Path\n",
    "\n",
    "# Create figs directory if it doesn't exist\n",
    "os.makedirs('figs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2f00c9",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de26d5f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "TRAIN_CSV = \"data/train.csv\"\n",
    "TEST_CSV = \"data/test_public_expanded.csv\"\n",
    "RESULTS_DIR = \"results\"\n",
    "THRESHOLD = 0.5  # Decision threshold for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0984f0cd",
   "metadata": {},
   "source": [
    "## Utility Function to List Identity Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a4d7ce",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def list_identity_columns(df):\n",
    "    \"\"\"\n",
    "    List all identity columns in the dataframe.\n",
    "    \n",
    "    Identity columns are typically those related to demographic groups.\n",
    "    In the Jigsaw dataset, these are columns like 'male', 'female', 'black', etc.\n",
    "    \"\"\"\n",
    "    # Common identity column patterns\n",
    "    identity_patterns = [\n",
    "        # Demographic groups\n",
    "        'male', 'female', 'transgender', 'other_gender', 'heterosexual', 'homosexual_gay_or_lesbian',\n",
    "        'bisexual', 'other_sexual_orientation', 'christian', 'jewish', 'muslim', 'hindu',\n",
    "        'buddhist', 'atheist', 'other_religion', 'black', 'white', 'asian', 'latino',\n",
    "        'other_race_or_ethnicity', 'physical_disability', 'intellectual_or_learning_disability',\n",
    "        'psychiatric_or_mental_illness', 'other_disability',\n",
    "        \n",
    "        # Sometimes these are prefixed\n",
    "        'identity_', 'demo_'\n",
    "    ]\n",
    "    \n",
    "    identity_cols = []\n",
    "    \n",
    "    # Check each column in the dataframe\n",
    "    for col in df.columns:\n",
    "        # Check if the column matches any of the identity patterns\n",
    "        if any(pattern in col.lower() for pattern in identity_patterns):\n",
    "            identity_cols.append(col)\n",
    "    \n",
    "    return identity_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed980849",
   "metadata": {},
   "source": [
    "## Generate Figure 1: Identity Prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6d08bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "# Get identity columns\n",
    "identity_cols = list_identity_columns(train_df)\n",
    "print(f\"Found {len(identity_cols)} identity columns: {identity_cols}\")\n",
    "\n",
    "# Generate the identity prevalence figure\n",
    "prevalence_fig = figure_utils.identity_prevalence(train_df, identity_cols)\n",
    "plt.close(prevalence_fig)  # Close the figure to free memory\n",
    "\n",
    "print(f\"Saved identity prevalence figure to figs/identity_prevalence.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9cdea8",
   "metadata": {},
   "source": [
    "## Process Each Model's Results and Generate Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c730735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all metrics files in the results directory\n",
    "metrics_files = glob.glob(os.path.join(RESULTS_DIR, \"metrics_*.csv\"))\n",
    "print(f\"Found {len(metrics_files)} metrics files\")\n",
    "\n",
    "# Initialize a list to store figure metadata\n",
    "figure_inventory = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f5a66b",
   "metadata": {},
   "source": [
    "## Loop Through Each Model's Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1b8ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metrics_file in metrics_files:\n",
    "    # Extract model name from the filename\n",
    "    model_name = os.path.basename(metrics_file).replace(\"metrics_\", \"\").replace(\".csv\", \"\")\n",
    "    print(f\"Processing model: {model_name}\")\n",
    "    \n",
    "    # Load metrics\n",
    "    try:\n",
    "        metrics_df = pd.read_csv(metrics_file)\n",
    "        print(f\"Loaded metrics data with shape: {metrics_df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading metrics file {metrics_file}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Check if this is a dry-run (all subgroup_auc values are NaN)\n",
    "    if 'subgroup_auc' in metrics_df.columns and metrics_df['subgroup_auc'].isna().all():\n",
    "        print(f\"Dry-run detected: skipping figure generation for model {model_name}\")\n",
    "        if not (fig_dir := Path('figs')).exists():\n",
    "            fig_dir.mkdir()\n",
    "        plt.figure(figsize=(4,2)); plt.text(0.5,0.5, f'Skipped {model_name}', ha='center', va='center')\n",
    "        plt.axis('off'); plt.savefig(fig_dir / f'skip_{model_name}.png'); plt.close()\n",
    "        figure_inventory.append(str(fig_dir / f'skip_{model_name}.png'))\n",
    "        continue\n",
    "    \n",
    "    # Check if the metrics file has the expected structure\n",
    "    required_columns = ['identity_group', 'metric_name', 'value']\n",
    "    if not all(col in metrics_df.columns for col in required_columns):\n",
    "        # If not, try to reshape the data to match the expected format\n",
    "        print(f\"Metrics file doesn't have the expected columns: {required_columns}\")\n",
    "        print(\"Attempting to reshape data...\")\n",
    "        \n",
    "        # Melt the DataFrame to convert it to the required format\n",
    "        try:\n",
    "            # Assuming first column is identity_group and other columns are metrics\n",
    "            id_col = metrics_df.columns[0]\n",
    "            metric_cols = metrics_df.columns[1:]\n",
    "            \n",
    "            metrics_df = pd.melt(\n",
    "                metrics_df, \n",
    "                id_vars=[id_col], \n",
    "                value_vars=metric_cols,\n",
    "                var_name='metric_name',\n",
    "                value_name='value'\n",
    "            )\n",
    "            metrics_df = metrics_df.rename(columns={id_col: 'identity_group'})\n",
    "            print(f\"Reshaped metrics data to: {metrics_df.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reshaping metrics data: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Load predictions if available\n",
    "    pred_file = os.path.join(RESULTS_DIR, f\"preds_{model_name}.csv\")\n",
    "    preds_df = None\n",
    "    \n",
    "    if os.path.exists(pred_file):\n",
    "        try:\n",
    "            preds_df = pd.read_csv(pred_file)\n",
    "            print(f\"Loaded predictions data with shape: {preds_df.shape}\")\n",
    "            \n",
    "            # Check if the predictions file has the expected structure (id, prediction)\n",
    "            if 'id' in preds_df.columns and 'prediction' in preds_df.columns:\n",
    "                pass\n",
    "            elif len(preds_df.columns) >= 2:\n",
    "                # Rename the columns to match expected format\n",
    "                preds_df = preds_df.iloc[:, :2]\n",
    "                preds_df.columns = ['id', 'prediction']\n",
    "            else:\n",
    "                print(f\"Predictions file doesn't have the expected structure\")\n",
    "                preds_df = None\n",
    "                \n",
    "            # If test_expanded file exists, merge with predictions\n",
    "            if os.path.exists(TEST_CSV) and preds_df is not None:\n",
    "                test_df = pd.read_csv(TEST_CSV)\n",
    "                if 'id' in test_df.columns:\n",
    "                    preds_df = pd.merge(preds_df, test_df, on='id', how='left')\n",
    "                    print(f\"Merged predictions with test data: {preds_df.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading predictions file {pred_file}: {e}\")\n",
    "            preds_df = None\n",
    "    \n",
    "    # Generate Figure 2: ROC Curve\n",
    "    if preds_df is not None and 'prediction' in preds_df.columns and 'target' in preds_df.columns:\n",
    "        roc_fig = figure_utils.roc_curve_figure(\n",
    "            preds_df['target'], \n",
    "            preds_df['prediction'], \n",
    "            model_name\n",
    "        )\n",
    "        plt.close(roc_fig)  # Close the figure to free memory\n",
    "        figure_inventory.append(f\"overall_roc_{model_name}.png\")\n",
    "        print(f\"Generated ROC curve figure for {model_name}\")\n",
    "    else:\n",
    "        print(f\"Skipping ROC curve figure for {model_name} (missing predictions or target)\")\n",
    "    \n",
    "    # Generate Figure 3: Fairness Heatmap (with updated settings)\n",
    "    heatmap_fig = figure_utils.fairness_heatmap(metrics_df, model_name)\n",
    "    plt.close(heatmap_fig)  # Close the figure to free memory\n",
    "    figure_inventory.append(f\"fairness_heatmap_{model_name}.png\")\n",
    "    print(f\"Generated fairness heatmap figure for {model_name}\")\n",
    "    \n",
    "    # Generate Figure 4: Power Mean Bar\n",
    "    power_fig = figure_utils.power_mean_bar(metrics_df, model_name)\n",
    "    plt.close(power_fig)  # Close the figure to free memory\n",
    "    figure_inventory.append(f\"power_mean_bar_{model_name}.png\")\n",
    "    print(f\"Generated power mean bar figure for {model_name}\")\n",
    "    \n",
    "    # Generate Figure 5: Grouped Bar by Identity (New)\n",
    "    grouped_bar_fig = figure_utils.grouped_bar_by_identity(metrics_df, model_name)\n",
    "    plt.close(grouped_bar_fig)  # Close the figure to free memory\n",
    "    figure_inventory.append(f\"grouped_bar_{model_name}.png\")\n",
    "    print(f\"Generated grouped bar by identity figure for {model_name}\")\n",
    "    \n",
    "    # Generate Figure 6: Worst K Bar (replaces worst k table)\n",
    "    worst_k_fig = figure_utils.worst_k_bar(metrics_df, model_name, k=5)\n",
    "    plt.close(worst_k_fig)  # Close the figure to free memory\n",
    "    figure_inventory.append(f\"worst_k_bar_{model_name}.png\")\n",
    "    print(f\"Generated worst k bar figure for {model_name}\")\n",
    "    \n",
    "    # Process error rate analysis if predictions are available\n",
    "    if preds_df is not None and 'prediction' in preds_df.columns and 'target' in preds_df.columns:\n",
    "        # Extract identity columns from the test data\n",
    "        test_identity_cols = list_identity_columns(preds_df)\n",
    "        \n",
    "        if test_identity_cols:\n",
    "            print(f\"Found {len(test_identity_cols)} identity columns in test data\")\n",
    "            \n",
    "            # Track worst performing subgroup\n",
    "            worst_group = None\n",
    "            worst_auc = 1.0\n",
    "            \n",
    "            # Calculate error rate gaps for each identity group\n",
    "            gaps_dict = {}\n",
    "            \n",
    "            for identity_col in test_identity_cols:\n",
    "                # Create identity mask\n",
    "                identity_mask = preds_df[identity_col] == 1\n",
    "                \n",
    "                # Skip identities with too few samples\n",
    "                if identity_mask.sum() < 50:\n",
    "                    print(f\"Skipping {identity_col} (insufficient samples: {identity_mask.sum()})\")\n",
    "                    continue\n",
    "                \n",
    "                # Calculate error rate gaps at threshold\n",
    "                fpr_gap, fnr_gap = threshold_utils.error_rate_gaps(\n",
    "                    preds_df['target'].values,\n",
    "                    preds_df['prediction'].values, \n",
    "                    identity_mask.values,\n",
    "                    thresh=THRESHOLD\n",
    "                )\n",
    "                \n",
    "                # Store for heatmap\n",
    "                gaps_dict[identity_col] = (fpr_gap, fnr_gap)\n",
    "                \n",
    "                # Find the worst performing group\n",
    "                if 'subgroup_auc' in metrics_df['metric_name'].unique():\n",
    "                    subgroup_metrics = metrics_df[\n",
    "                        (metrics_df['metric_name'] == 'subgroup_auc') & \n",
    "                        (metrics_df['identity_group'] == identity_col)\n",
    "                    ]\n",
    "                    if not subgroup_metrics.empty:\n",
    "                        auc_value = subgroup_metrics['value'].iloc[0]\n",
    "                        if auc_value < worst_auc:\n",
    "                            worst_auc = auc_value\n",
    "                            worst_group = identity_col\n",
    "                \n",
    "                # Generate threshold sweep curve for current identity\n",
    "                # First compute the sweep data\n",
    "                df_sweep = threshold_utils.sweep_threshold_gaps(\n",
    "                    preds_df['target'].values,\n",
    "                    preds_df['prediction'].values, \n",
    "                    identity_mask.values,\n",
    "                    n_steps=101\n",
    "                )\n",
    "                \n",
    "                # Generate and save the curve plot\n",
    "                curve_fig = figure_utils.threshold_gap_curve(df_sweep, identity_col, model_name)\n",
    "                plt.close(curve_fig)  # Close the figure to free memory\n",
    "                figure_inventory.append(f\"threshold_gap_curve_{identity_col}_{model_name}.png\")\n",
    "                \n",
    "            # Generate error gap heatmap with all identity groups\n",
    "            if gaps_dict:\n",
    "                error_heatmap_fig = figure_utils.error_gap_heatmap(\n",
    "                    model_name,\n",
    "                    gaps_dict,\n",
    "                    threshold=THRESHOLD\n",
    "                )\n",
    "                plt.close(error_heatmap_fig)  # Close the figure to free memory\n",
    "                figure_inventory.append(f\"error_gap_heatmap_{model_name}.png\")\n",
    "                print(f\"Generated error gap heatmap for {model_name}\")\n",
    "            \n",
    "            # Generate confusion mosaic for worst performing identity group\n",
    "            if worst_group:\n",
    "                print(f\"Worst performing identity group: {worst_group} (AUC: {worst_auc:.3f})\")\n",
    "                \n",
    "                # Create binary predictions at threshold 0.5\n",
    "                binary_preds = (preds_df['prediction'] >= THRESHOLD).astype(int)\n",
    "                \n",
    "                # Create confusion mosaic\n",
    "                worst_mask = preds_df[worst_group] == 1\n",
    "                \n",
    "                mosaic_fig = figure_utils.add_confusion_mosaic(\n",
    "                    preds_df['target'].values,\n",
    "                    binary_preds.values,\n",
    "                    worst_mask.values,\n",
    "                    identity_name=worst_group,\n",
    "                    model_name=model_name\n",
    "                )\n",
    "                plt.close(mosaic_fig)  # Close the figure to free memory\n",
    "                figure_inventory.append(f\"confusion_{worst_group}_{model_name}.png\")\n",
    "                print(f\"Generated confusion mosaic for worst group {worst_group}\")\n",
    "            \n",
    "    # Generate original threshold sweep if needed\n",
    "    if preds_df is not None and 'prediction' in preds_df.columns and 'target' in preds_df.columns:\n",
    "        test_identity_cols = list_identity_columns(preds_df)\n",
    "        \n",
    "        if test_identity_cols:\n",
    "            # Create identity arrays for the threshold sweep\n",
    "            identity_arrays = [preds_df[col].values for col in test_identity_cols]\n",
    "            \n",
    "            threshold_fig = figure_utils.threshold_sweep(\n",
    "                preds_df['target'],\n",
    "                preds_df['prediction'],\n",
    "                identity_arrays,\n",
    "                model_name\n",
    "            )\n",
    "            plt.close(threshold_fig)  # Close the figure to free memory\n",
    "            figure_inventory.append(f\"threshold_sweep_{model_name}.png\")\n",
    "            print(f\"Generated threshold sweep figure for {model_name}\")\n",
    "        else:\n",
    "            print(f\"Skipping threshold sweep figure for {model_name} (no identity columns in test data)\")\n",
    "    else:\n",
    "        print(f\"Skipping threshold sweep figure for {model_name} (missing predictions or target)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d64e4d1",
   "metadata": {},
   "source": [
    "## Generate Figure 7: Before vs After Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2867a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find baseline and improved model metrics\n",
    "baseline_metrics_file = None\n",
    "improved_metrics_file = None\n",
    "\n",
    "# Look for baseline model metrics\n",
    "for metrics_file in metrics_files:\n",
    "    if \"baseline\" in metrics_file.lower():\n",
    "        baseline_metrics_file = metrics_file\n",
    "        break\n",
    "\n",
    "# If no explicit baseline, use the first metrics file\n",
    "if baseline_metrics_file is None and metrics_files:\n",
    "    baseline_metrics_file = metrics_files[0]\n",
    "\n",
    "# Look for improved model metrics (tfidf_lr_full is specified in the requirements)\n",
    "for metrics_file in metrics_files:\n",
    "    if \"tfidf_lr_full\" in metrics_file.lower():\n",
    "        improved_metrics_file = metrics_file\n",
    "        break\n",
    "\n",
    "# If no explicit improved model, use the last metrics file (different from baseline)\n",
    "if improved_metrics_file is None and len(metrics_files) > 1:\n",
    "    improved_metrics_file = [f for f in metrics_files if f != baseline_metrics_file][0]\n",
    "\n",
    "if baseline_metrics_file and improved_metrics_file and baseline_metrics_file != improved_metrics_file:\n",
    "    print(f\"Generating before vs after scatter plot:\")\n",
    "    print(f\"  Baseline: {os.path.basename(baseline_metrics_file)}\")\n",
    "    print(f\"  Improved: {os.path.basename(improved_metrics_file)}\")\n",
    "    \n",
    "    # Load the metrics DataFrames\n",
    "    baseline_df = pd.read_csv(baseline_metrics_file)\n",
    "    improved_df = pd.read_csv(improved_metrics_file)\n",
    "    \n",
    "    # Check if the metrics files have the expected structure\n",
    "    required_columns = ['identity_group', 'metric_name', 'value']\n",
    "    \n",
    "    for df, name in [(baseline_df, \"baseline\"), (improved_df, \"improved\")]:\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            # If not, try to reshape the data to match the expected format\n",
    "            print(f\"{name} metrics file doesn't have the expected columns: {required_columns}\")\n",
    "            print(\"Attempting to reshape data...\")\n",
    "            \n",
    "            # Melt the DataFrame to convert it to the required format\n",
    "            try:\n",
    "                # Assuming first column is identity_group and other columns are metrics\n",
    "                id_col = df.columns[0]\n",
    "                metric_cols = df.columns[1:]\n",
    "                \n",
    "                reshaped_df = pd.melt(\n",
    "                    df, \n",
    "                    id_vars=[id_col], \n",
    "                    value_vars=metric_cols,\n",
    "                    var_name='metric_name',\n",
    "                    value_name='value'\n",
    "                )\n",
    "                reshaped_df = reshaped_df.rename(columns={id_col: 'identity_group'})\n",
    "                \n",
    "                if name == \"baseline\":\n",
    "                    baseline_df = reshaped_df\n",
    "                else:\n",
    "                    improved_df = reshaped_df\n",
    "                    \n",
    "                print(f\"Reshaped {name} metrics data successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reshaping {name} metrics data: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Get common identity groups\n",
    "    baseline_identity_groups = baseline_df['identity_group'].unique()\n",
    "    improved_identity_groups = improved_df['identity_group'].unique()\n",
    "    common_identity_groups = list(set(baseline_identity_groups) & set(improved_identity_groups))\n",
    "    \n",
    "    if common_identity_groups:\n",
    "        scatter_fig = figure_utils.before_vs_after_scatter(\n",
    "            baseline_df,\n",
    "            improved_df,\n",
    "            common_identity_groups\n",
    "        )\n",
    "        plt.close(scatter_fig)  # Close the figure to free memory\n",
    "        figure_inventory.append(\"before_vs_after_scatter.png\")\n",
    "        print(f\"Generated before vs after scatter plot\")\n",
    "    else:\n",
    "        print(\"No common identity groups found between baseline and improved models\")\n",
    "else:\n",
    "    print(\"Skipping before vs after scatter plot (couldn't find suitable baseline and improved models)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034dc65b",
   "metadata": {},
   "source": [
    "## Save Figure Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c057db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the figure inventory\n",
    "inventory = {\n",
    "    \"figures\": figure_inventory,\n",
    "    \"count\": len(figure_inventory)\n",
    "}\n",
    "\n",
    "with open('figs/figure_inventory.json', 'w') as f:\n",
    "    json.dump(inventory, f, indent=2)\n",
    "\n",
    "print(f\"Saved figure inventory to figs/figure_inventory.json\")\n",
    "print(f\"Generated {len(figure_inventory)} figures in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40e2909",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has generated the following figures:\n",
    "\n",
    "1. `identity_prevalence.png` - Bar plot showing prevalence of identity groups\n",
    "2. `overall_roc_{model}.png` - ROC curves for each model\n",
    "3. `fairness_heatmap_{model}.png` - Heatmaps of fairness metrics for each model\n",
    "4. `power_mean_bar_{model}.png` - Bar plots of power mean differences for each model\n",
    "5. `grouped_bar_{model}.png` - Grouped bar plots of metrics by identity group\n",
    "6. `worst_k_bar_{model}.png` - Bar charts of worst-performing identity groups\n",
    "7. `threshold_sweep_{model}.png` - Threshold sweep analysis for each model\n",
    "8. `threshold_gap_curve_{identity}_{model}.png` - FPR/FNR gap curves by threshold\n",
    "9. `error_gap_heatmap_{model}.png` - Heatmap of FPR/FNR gaps across groups at Ï„=0.5\n",
    "10. `confusion_{identity}_{model}.png` - Confusion matrix mosaic for worst subgroup\n",
    "11. `before_vs_after_scatter.png` - Scatter plot comparing baseline and improved models\n",
    "\n",
    "All figures are saved in the `figs/` directory. "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "executable": "/usr/bin/env python",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
